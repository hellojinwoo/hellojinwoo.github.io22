{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"01.Maximum_Likelihood_Estimation/","title":"Maximum Likelihood Estimation (MLE)","text":"<p>Source:  Probability concepts explained: Maximum likelihood estimation</p> <ul> <li>Notion article. Written in English.</li> <li>Discussed the import points, but often overlooked, in a plain manner, such as \"the difference between likelihood and probability\" and \"when MLE is the same as the minimizing the residuam sum of squared\".  </li> </ul> <p>MLE</p> <ul> <li>Blog article. Written in Korean.</li> <li>Has intuitive illustrations, which is very helpful for understanding the concepts of MLE</li> </ul>"},{"location":"01.Maximum_Likelihood_Estimation/#maximum-likelihood-estimation-mle","title":"Maximum Likelihood Estimation (MLE)\u00b6","text":""},{"location":"01.Maximum_Likelihood_Estimation/#what-is-mle","title":"What is MLE?\u00b6","text":"<p>Maximum likelihood estimation is a method that determines values for the parameters of a model. The parameter values are found such that they maximise the likelihood that the process described by the model produced the data that were actually observed.</p>"},{"location":"01.Maximum_Likelihood_Estimation/#what-is-parameter","title":"What is parameter?\u00b6","text":"<p>In statistics, a parameter represents a fixed value that describes a characteristic of a relationship between variables. Imagine you're trying to understand the relationship between the number of rooms in a house and its price. You might think that there's a hidden, fixed relationship that determines how these two factors are related, like a universal rule.</p> <p>For example, the relationship might look like: (house price) = \u20ac5,000 x (number of rooms) + \u20ac3,000. In this case, the \u20ac5,000 represents the increase in the house price for each additional room, and the \u20ac3,000 represents the baseline house price when there are no rooms. These values before the varaible, \u20ac5,000 and \u20ac3,000, are called parameters.</p> <p>Parameters help us quantify and understand the underlying patterns in the relationships between variables, like the number of rooms and house prices in our example. By estimating these parameters from data, we can make predictions and draw insights about the relationship we're studying</p>"},{"location":"01.Maximum_Likelihood_Estimation/#why-is-it-preferred-over-others","title":"Why is it preferred over others?\u00b6","text":"<p>The coefficients of MLE are desirable for the following reasons:</p> <ul> <li>Consistency: As the sample size(n) increases, MLE estimates converge to the true parameter values. This means that with more data, MLE estimates become more accurate.</li> <li>Asymptotic Normality: Under certain regularity conditions, MLE estimates are approximately normally distributed when the sample size is large. This property allows us to construct confidence intervals and perform hypothesis tests using standard techniques.</li> </ul>"},{"location":"02.Simple_Linear_Regression_Overview/","title":"Simple Linear Regression","text":"<p>Source</p> <ol> <li>An Introduction to Statistical Learning</li> <li>The Effect: An Introduction to Research Design and Causality</li> </ol>"},{"location":"02.Simple_Linear_Regression_Overview/#simple-linear-regression","title":"Simple Linear Regression\u00b6","text":""},{"location":"02.Simple_Linear_Regression_Overview/#1-what-is-simple-linear-regression","title":"1. What is Simple Linear Regression?\u00b6","text":"<ul> <li>Simple linear regression lives up to its name: It is a very straightforward approach for predicting a quantitative response Y on the basis of a single predictor variable X.<ul> <li>It assumes that there is approximately a linear relationship between X and Y. Mathematically, we can write this linear relationship as ${Y}$ \u2248 ${\\beta}_0$ + ${\\beta}_1$ * X</li> <li>${\\beta}_0$ and ${\\beta}_1$ are two unknown constants that represent the intercept and slope terms in the linear model.</li> <li>Once we have used our data to produce estimates  $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$, we can predict future Y on the basis of a particular value of X. Here we use a hat symbol, to denote the estimated value for an unknown parameter or coefficient, or to denote the predicted value of the response.</li> </ul> </li> </ul> $$ \\hat{Y} \u2248 \\hat{\\beta}_0 + \\hat{\\beta}_1 * X \\qquad (1.1) $$<ul> <li>We use the symbol \"\u2248\" instead of \"=\", because this simple linear equation will rarely predict any observation perfectly, much less all of the observations.<ul> <li>There\u2019s going to be a difference between the line that we fit and the observation we get. We can add this difference to our actual equation as an \u201cerror term\u201d, which is ${e}$</li> </ul> </li> </ul> $$ {Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1  * X + \\epsilon \\qquad (1.2) $$"},{"location":"02.Simple_Linear_Regression_Overview/#2-how-to-estimate-the-coefficients-of-simple-linear-regression-model","title":"2. How to Estimate the Coefficients of Simple Linear Regression Model?\u00b6","text":""},{"location":"02.Simple_Linear_Regression_Overview/#21-introdction-of-ordinay-least-sqaures-ols","title":"2.1) Introdction of Ordinay Least sqaures (OLS)\u00b6","text":"<ul> <li><p>In practice, ${\\beta}_0$ and ${\\beta}_1$ are unknown, so we need to estimate these coefficients using the data that we have in our hands.</p> <ul> <li>The goal is to obtain coefficient estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ such that the linear model fits the available data well \u2014 that is, so that $\\hat{Y}$ \u2248 $\\hat{\\beta}_0$ + $\\hat{\\beta}_1$ * X for i = 1, ..., n. In other words, we want to find an intercept $\\hat{\\beta}_0$ and a slope $\\hat{\\beta}_1$ such that the resulting line is as close as possible to the available data points.</li> </ul> </li> <li><p>There are a number of ways of measuring closeness. By far the most common approach is OLS, which minimizes the least squares criterion, rather than other methods such as sum of absolute residuals.</p> </li> <li><p>Minimizing the least squares criterion means minimizing the residual sum of squares: RSS = ${e}^2_1$ + ${e}^2_2$ + ... + ${e}^2_n$</p> <ul> <li>Let $\\hat{y}_i$ = $\\hat{\\beta}_0$ + $\\hat{\\beta}_1$ * ${x}_i$ be the prediction for Y based on the ${i}$ th value of X.<ul> <li>Here, X and Y are variables. For instance, X is weight, while Y is height. ${x}_i$ and ${y}_i$ are the values of each observation. For example, ${x}_1$ is John's weight and ${y}_1$ is his height.</li> </ul> </li> <li>Then ${e}_i$ = ${y}_i$ \u2212 $\\hat{y}_i$ represents the ${i}$ th residual \u2014 this is the difference between the ${i}$ th observed response value and the ${i}$ th response value that is predicted by our linear model.</li> <li>The OLS approach chooses ${\\beta}_0$ and ${\\beta}_1$ which minimize the RSS, which are shown as below:<ul> <li>$\\bar{y}$ and $\\bar{x}$ are the sample means ($\\bar{y} = \\sum_{i=1}^{n} y_i$ and $\\bar{x} = \\sum_{i=1}^{n} x_i$)</li> </ul> </li> </ul> </li> </ul> $$ \\hat\\beta_1 = \\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} \\qquad (2.1) $$$$ \\hat\\beta_0 = \\bar{y} - \\hat\\beta_1\\bar{x}  \\qquad (2.2) $$<ul> <li>Please refer to Khan academy's video \"minimizing squared error to regression line\" to learn how this formula is derived.</li> </ul>"},{"location":"02.Simple_Linear_Regression_Overview/#22-why-minimizing-the-rss-is-by-far-the-most-widely-used","title":"2.2) Why Minimizing the RSS is by far the most widely used?\u00b6","text":"<p>1. Mathematically convenient</p> <ul> <li>The sum of squared residuals is a quadratic function, which means that it has a unique minimum point.</li> <li>This allows us to use calculus to find the minimum point, which corresponds to the estimates of the regression coefficients that minimize the sum of squared residuals. </li> </ul> <p>2. Leading to the Maximul Likelihood Estimation(MLE) of coefficients 2.1) What is MLE and why is it important?</p> <ul> <li><p>In statistics, MLE is a method of estimating the parameters of a probability distribution, given some observed data.</p> <ul> <li>This is achieved by maximizing a likelihood function so that, under the assumed statistical model, the observed data is most probable.</li> <li>It is calculated by taking the product of the probability density functions for each data point. For the following example where we have 2 coefficients, $L(\\beta_0, \\beta_1)$ is a likelihood function and $f(y_i | x_i, \\beta_0, \\beta_1)$ is the probability density function, calculating the probability of each data point.</li> </ul> <p>\\begin{align*}   L(\\beta_0, \\beta_1) &amp;= \\prod_{i=1}^n f(y_i | x_i, \\beta_0, \\beta_1) \\   \\end{align*}</p> </li> <li><p>The coefficients of MLE are desirable for the following reasons:</p> <ol> <li>Consistency: As the sample size(n) increases, MLE estimates converge to the true parameter values. This means that with more data, MLE estimates become more accurate.</li> <li>Asymptotic Normality: Under certain regularity conditions, MLE estimates are approximately normally distributed when the sample size is large. This property allows us to construct confidence intervals and perform hypothesis tests using standard techniques.</li> </ol> </li> </ul> <p> 2.2) Why OLS estimator is the same thing as MLE estimator?</p> <ul> <li>This all starts with the assumption that the residual term ($\u03b5$), which is in the simple linear regression model $Y = \u03b2\u2080 + \u03b2\u2081X + \u03b5$, follows a normal distribution with a mean of 0<ul> <li>First, why the normal distribution?<ul> <li>Given a fixed mean and variance, the normal distribution has the highest entropy among all continuous distributions, meaning it carries the least amount of additional information.</li> <li>In the absence of more detailed knowledge about the residual distribution, the normal distribution represents the most \"uninformative\" or \"agnostic\" choice.</li> </ul> </li> <li>Second, why the mean of 0?<ul> <li>If the linear regression model is correctly specified, meaning it includes all relevant predictors and their true functional forms, then the error term should capture only random fluctuations not explained by the predictors.</li> <li>In this case, it's reasonable to expect that the average error (i.e., the mean of the residual term) should be close to 0, as there should be no systematic bias in the predictions.</li> </ul> </li> </ul> </li> </ul> <ul> <li>Before going furether, let's review the probability density function of normal distribution.</li> </ul> Figure 1.1. The Normal Distribution Density Function. <ul> <li>With this normal distribution and means as 0 assumption of the residual term, the likelihood of observing a particular data point (${x}_\u1d62$ , ${y}_\u1d62$) given the model's parameters can be expressed using the probability density function of the normal distribution:</li> </ul> $$ \\begin{align} L_i(\\beta_0, \\beta_1) &amp;= \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(e - \\mu)^2}{2\\sigma^2}} \\\\                       &amp;= \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(y_i - (\\beta_0 + \\beta_1x_i))^2}{2\\sigma^2}} \\end{align} $$<ul> <li>Let's focus on the expontent of the base of the natural logarithm.</li> </ul> $$ {-\\frac{(e - \\mu)^2}{2\\sigma^2}} = {-\\frac{(y_i - (\\beta_0 + \\beta_1x_i))^2}{2\\sigma^2}} $$<ul> <li><p>The residual term represents the difference between the observation ($Y$) and the model's predicted value ($\\hat{Y}$).</p> <ul> <li>In math equation, $\u03b5 = Y - (\u03b2\u2080 + \u03b2\u2081X)$</li> <li>For each observation ${i}$, we have: ${\u03b5}_i = {y}_i - ({\u03b2}_0 + {\u03b2}_1 * {x}_i)$</li> </ul> </li> <li><p>The normal distribution density function is as follows:</p> </li> <li><p>Since we assume that the error term follows a normal distribution with mean 0 and variance ${\\sigma}^2$, the likelihood of observing a particular data point $(x\u1d62, y\u1d62)$ given the model's parameters can be expressed using the probability density function of the normal distribution</p> </li> </ul> $$ L\u1d62(\\beta_0, \\beta_1) = (1 / (\u221a(2\u03c0\u03c3\u00b2))) * exp(-(\u03b5\u1d62)\u00b2 / (2\u03c3\u00b2)) $$"},{"location":"02.Simple_Linear_Regression_Overview/#3-assessing-the-accuracy-of-the-coefficient-estimates","title":"3. Assessing the Accuracy of the Coefficient Estimates\u00b6","text":"<ul> <li>we assume that the true relationship between X and Y takes the form ${Y} = {f(X)} + {\u03f5}$ for some unknown function ${f}$, where ${\u03f5}$ is a mean-zero random error term. If ${f}$ is to be approximated by a linear function, then we can write this relationship as</li> </ul> $$ Y = \\beta_0 + \\beta_1X + \\epsilon \\qquad (3.1) $$<ul> <li>Here ${\\beta}_0$ is the intercept term-that is, the expected value of ${Y}$ when ${X}$ = 0, and ${\\beta}_1$ is the slope\u2014the average increase in ${Y}$ associated with a one-unit increase in ${X}$. The error term is a catch-all for what we miss with this simple model. We typically assume that the error term is independent of ${X}$.</li> <li>This linear regression model (3.1) defines the population regression line, which is the best linear approximation to the true relationship between ${X}$ and ${Y}$. The least squares regression coefficient estimates (2.1) &amp; (2.2) characterize the least squares line (3.1). In essence, the population regression line acts as the underlying function that generates the observations we encounter in real-life through samplings, such as surveys and polling. The left-hand panel of Figure 3.1, shown below, displays these two lines in a simple simulated example.</li> </ul> Figure 3.1. A simulated data set. Left: The red line represents the true relationship, f(X) = 2+3X, which is known as the population regression line. The blue line is the least squares line; it is the least squares estimate for f(X) based on the observed data, shown in black. Right: The population regression line is again shown in red, and the least squares line in dark blue. In light blue, ten least squares lines are shown, each computed on the basis of a separate random set of observations. Each least squares line is different, but on average, the least squares lines are quite close to the population regression line. <ul> <li>We created 100 random Xs, and generated 100 corresponding Ys from the model</li> </ul> $$ Y = 2+3X + \u03f5 \\qquad (3.2) $$<ul> <li>${\u03f5}$ was generated from a normal distribution with mean zero. The red line in the left-hand panel of Figure 3.3 displays the true relationship, ${f(X)} = 2+3{X}$, while the blue line is the least squares stimate based on the observed data.</li> <li>The true relationship is generally not known for real data, but the least squares line can always be computed using the coefficient estimates given in (2.1) &amp; (2.2).<ul> <li>In other words, in real applications, we have access to a set of observations from which we can compute the least squares line; however, the population regression line is unobserved.</li> <li>This means that in order to determine the linear relationship between ${X}$ and ${Y}$, such as weight and height, we can use the least squares line computed using equations (2.1) and (2.2). However, we are unable to know the true linear relationship, which is represented by the population regression line.</li> </ul> </li> <li>In the right-hand panel of Figure 3.1 we have generated ten different data sets from the model given by (3.1) and plotted the corresponding ten least squares lines.<ul> <li>Notice that different data sets generated from the same true model result in slightly different least squares lines, but the unobserved population regression line does not change.</li> </ul> </li> </ul>"}]}